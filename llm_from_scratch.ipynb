{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install datasets transformers matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Load the SQuAD dataset using Hugging Face Datasets\n",
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "# Function to process data (modify for specific needs)\n",
    "def process_data(data):\n",
    "  content_question_pairs = []\n",
    "  for example in data:\n",
    "    content_question_pairs.append(example[\"context\"] + \"\\n\" + example[\"question\"] + \"\\n\" + example[\"answers\"][\"text\"][0] + \"[END]\")\n",
    "  return content_question_pairs\n",
    "\n",
    "# Process training and validation data\n",
    "train_data_processed = \"\\n\".join(process_data(squad[\"train\"]))\n",
    "validation_data_processed = process_data(squad[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "  \"\"\"\n",
    "  This function tokenizes a string by splitting it on whitespace after\n",
    "  removing all non-alphanumeric characters and converting everything to lowercase.\n",
    "\n",
    "  Args:\n",
    "      text: The string to tokenize.\n",
    "\n",
    "  Returns:\n",
    "      A list of the individual words in the tokenized string.\n",
    "  \"\"\"\n",
    "  # Remove non-alphanumeric characters and convert to lowercase\n",
    "  alphanumeric_text = \"\".join(char.lower() for char in text if char.isalnum() or char.isspace())\n",
    "  # Split the text on whitespace\n",
    "  tokens = alphanumeric_text.split()\n",
    "  return tokens\n",
    "\n",
    "# Example usage\n",
    "text = \"This is some text with! punctuation & symbols.\"\n",
    "tokens = tokenizer(train_data_processed)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_LENGTH = 10\n",
    "\n",
    "import pandas as pd\n",
    "def generate_training_pairs(tokens):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in range(0, len(tokens) - WINDOW_LENGTH):\n",
    "        inputs.append(tokens[i:i+WINDOW_LENGTH])\n",
    "        outputs.append([tokens[i+WINDOW_LENGTH]])\n",
    "    return inputs, outputs\n",
    "\n",
    "training_data_X, training_data_y = generate_training_pairs(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_X[0], training_data_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmbeddingModel(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tokens_to_int = {}\n",
    "        self.int_to_tokens = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tokens =  set([item for sublist in X + y for item in sublist]) \n",
    "        self.tokens_to_int = dict(zip(tokens,range(0,len(tokens))))\n",
    "        self.int_to_tokens = dict(zip(range(0,len(tokens)),tokens))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [[self.tokens_to_int.get(x,self.int_to_tokens.get(x)) for x in xs] for xs in X]\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"sentence_embedding\",\n",
    "            EmbeddingModel(),\n",
    "        ), \n",
    "        (\n",
    "            \"language_model\",\n",
    "            RandomForestClassifier(n_estimators=100, max_depth=10) \n",
    "        ),  \n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline.fit(training_data_X[0:10000], training_data_y[0:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data_X[10001],training_data_y[10001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict([training_data_X[10001]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(pipeline[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.transform([[\"hello\",\"world\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
